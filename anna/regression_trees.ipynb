{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ID_434.csv', 'ID_143.csv', 'ID_296.csv', 'ID_135.csv',\n",
       "       'ID_485.csv', 'ID_772.csv', 'ID_804.csv', 'ID_437.csv',\n",
       "       'ID_336.csv', 'ID_379.csv', 'ID_479.csv', 'ID_356.csv',\n",
       "       'ID_731.csv', 'ID_589.csv', 'ID_47.csv', 'ID_647.csv',\n",
       "       'ID_330.csv', 'ID_620.csv', 'ID_680.csv', 'ID_27.csv',\n",
       "       'ID_326.csv', 'ID_611.csv', 'ID_191.csv', 'ID_112.csv',\n",
       "       'ID_437.csv', 'ID_449.csv', 'ID_787.csv', 'ID_105.csv',\n",
       "       'ID_641.csv', 'ID_867.csv', 'ID_166.csv', 'ID_586.csv',\n",
       "       'ID_436.csv', 'ID_411.csv', 'ID_248.csv', 'ID_524.csv',\n",
       "       'ID_573.csv', 'ID_173.csv', 'ID_520.csv', 'ID_466.csv',\n",
       "       'ID_648.csv', 'ID_181.csv', 'ID_179.csv', 'ID_816.csv',\n",
       "       'ID_676.csv', 'ID_45.csv', 'ID_663.csv', 'ID_731.csv',\n",
       "       'ID_697.csv', 'ID_87.csv', 'ID_811.csv', 'ID_742.csv',\n",
       "       'ID_749.csv', 'ID_52.csv', 'ID_153.csv', 'ID_50.csv', 'ID_387.csv',\n",
       "       'ID_510.csv', 'ID_643.csv', 'ID_95.csv', 'ID_194.csv',\n",
       "       'ID_307.csv', 'ID_749.csv', 'ID_359.csv', 'ID_121.csv',\n",
       "       'ID_14.csv', 'ID_48.csv', 'ID_426.csv', 'ID_868.csv', 'ID_174.csv',\n",
       "       'ID_201.csv', 'ID_535.csv', 'ID_873.csv', 'ID_250.csv',\n",
       "       'ID_372.csv', 'ID_526.csv', 'ID_500.csv', 'ID_190.csv',\n",
       "       'ID_505.csv', 'ID_272.csv', 'ID_119.csv', 'ID_709.csv',\n",
       "       'ID_704.csv', 'ID_62.csv', 'ID_481.csv', 'ID_86.csv', 'ID_168.csv',\n",
       "       'ID_452.csv', 'ID_297.csv', 'ID_564.csv', 'ID_525.csv',\n",
       "       'ID_713.csv', 'ID_649.csv', 'ID_298.csv', 'ID_526.csv',\n",
       "       'ID_605.csv', 'ID_389.csv', 'ID_538.csv', 'ID_58.csv',\n",
       "       'ID_578.csv'], dtype='<U10')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# List all files in dir\n",
    "files = os.listdir('/Users/annastonek/Desktop/Studium Unterlagen/AI_ML_Climate_Change/LamaH-CE/A_basins_total_upstrm/2_timeseries/daily')\n",
    "\n",
    "# Select 0.5 of the files randomly \n",
    "random_files = np.random.choice(files, 100)\n",
    "random_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get CSV files list from a folder\n",
    "path = '/Users/annastonek/Desktop/Studium Unterlagen/AI_ML_Climate_Change/LamaH-CE/A_basins_total_upstrm/2_timeseries/daily/'\n",
    "csv_files = []\n",
    "for x in random_files:\n",
    "    csv_files.append(os.path.join(path, x))\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "counter = 0\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    aux = pd.read_csv(file, sep=';')\n",
    "    aux['Region'] = counter\n",
    "    df_list.append(aux)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we will create a regression tree from every dataset in the list and calculate the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "test_values = []\n",
    "for df in df_list:\n",
    "    # separate the target column from the feature columns\n",
    "    y = df[['prec']]\n",
    "    X = df.drop(['prec', '2m_temp_min', '2m_temp_max', '2m_dp_temp_min', '2m_dp_temp_mean', '2m_dp_temp_max'], axis=1)\n",
    "    # split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # preprocessing - scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train[['2m_temp_mean', '10m_wind_u', '10m_wind_v',\n",
    "       'fcst_alb', 'lai_high_veg', 'lai_low_veg', 'swe',\n",
    "       'surf_net_solar_rad_max', 'surf_net_solar_rad_mean',\n",
    "       'surf_net_therm_rad_max', 'surf_net_therm_rad_mean', 'surf_press',\n",
    "       'total_et', 'volsw_123', 'volsw_4']] = scaler.fit_transform(X_train[['2m_temp_mean', '10m_wind_u', '10m_wind_v',\n",
    "       'fcst_alb', 'lai_high_veg', 'lai_low_veg', 'swe',\n",
    "       'surf_net_solar_rad_max', 'surf_net_solar_rad_mean',\n",
    "       'surf_net_therm_rad_max', 'surf_net_therm_rad_mean', 'surf_press',\n",
    "       'total_et', 'volsw_123', 'volsw_4']])\n",
    "\n",
    "    y_train = scaler.fit_transform(y_train)\n",
    "\n",
    "    X_test[['2m_temp_mean', '10m_wind_u', '10m_wind_v',\n",
    "       'fcst_alb', 'lai_high_veg', 'lai_low_veg', 'swe',\n",
    "       'surf_net_solar_rad_max', 'surf_net_solar_rad_mean',\n",
    "       'surf_net_therm_rad_max', 'surf_net_therm_rad_mean', 'surf_press',\n",
    "       'total_et', 'volsw_123', 'volsw_4']] = scaler.fit_transform(X_test[['2m_temp_mean', '10m_wind_u', '10m_wind_v',\n",
    "       'fcst_alb', 'lai_high_veg', 'lai_low_veg', 'swe',\n",
    "       'surf_net_solar_rad_max', 'surf_net_solar_rad_mean',\n",
    "       'surf_net_therm_rad_max', 'surf_net_therm_rad_mean', 'surf_press',\n",
    "       'total_et', 'volsw_123', 'volsw_4']])\n",
    "\n",
    "    y_test = scaler.fit_transform(y_test)\n",
    "    \n",
    "    #build tree\n",
    "    tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "    #calculate predictions\n",
    "    y_pred = tree.predict(X_test)\n",
    "    \n",
    "    predictions.append(y_pred)\n",
    "    test_values.append(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.004984132715314812\n",
      "RMSE:  0.07059839031674031\n"
     ]
    }
   ],
   "source": [
    "y_pred = pd.DataFrame(predictions)\n",
    "\n",
    "y_true = []\n",
    "for i in range(len(test_values)):\n",
    "    y_true.append(np.array(test_values[0]).flatten())\n",
    "\n",
    "y_true = pd.DataFrame(y_true)\n",
    "print(\"MSE: \", mean_squared_error(y_true.mean(), y_pred.mean()))\n",
    "print(\"RMSE: \", math.sqrt(mean_squared_error(y_true.mean(), y_pred.mean())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}